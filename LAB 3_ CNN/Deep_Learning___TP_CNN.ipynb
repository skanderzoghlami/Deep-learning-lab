{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7046dc8c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1>Deep learning Practical Assignment 3:\n",
    "<h6>Convolutional Neural Networks\n",
    "<h4>Lab date : December 13th, 2021\n",
    "<h4>Deadline to submit : December 31th, 2021 - 11:59 PM (23:59)\n",
    "</div>\n",
    "<h3>Group Members: <ul>\n",
    "    <li> Ahmed Belaaj\n",
    "    <li> Omar Chaabouni\n",
    "    <li> Linna Azaiez\n",
    "    </ul>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b33c1e",
   "metadata": {},
   "source": [
    "### Practical assignment objective\n",
    "• To create a CNN based model.<br>\n",
    "• Train different model on different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124a23d",
   "metadata": {},
   "source": [
    "## Installing required packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f938a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from sklearn.datasets import load_digits\n",
    "import keras\n",
    "from sklearn.datasets import load_iris\n",
    "from keras.metrics import Precision , Recall\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D , Flatten, Dense, Dropout, Activation,  GlobalAveragePooling2D, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.layers import Dense\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from keras import layers , models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309bcd0",
   "metadata": {},
   "source": [
    "## importing datasets \n",
    "##### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2775803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f3500",
   "metadata": {},
   "source": [
    "##### Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371f0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff74c0",
   "metadata": {},
   "source": [
    "##### Olivetti dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "409bde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a740be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487af7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "train_labels = to_categorical(y_train, num_classes=digits.target_names.shape[0])\n",
    "test_labels = to_categorical(y_test, num_classes=digits.target_names.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce2fc5",
   "metadata": {},
   "source": [
    "## Work with fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc57261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144/144 [==============================] - 0s 775us/step - loss: 2.6862 - accuracy: 0.2477 - precision_2: 0.2778 - recall_2: 0.0835    \n",
      "Epoch 2/20\n",
      "144/144 [==============================] - 0s 755us/step - loss: 1.4299 - accuracy: 0.4920 - precision_2: 0.7086 - recall_2: 0.2589\n",
      "Epoch 3/20\n",
      "144/144 [==============================] - 0s 651us/step - loss: 0.9680 - accuracy: 0.6896 - precision_2: 0.8450 - recall_2: 0.4892\n",
      "Epoch 4/20\n",
      "144/144 [==============================] - 0s 675us/step - loss: 0.7216 - accuracy: 0.7857 - precision_2: 0.8740 - recall_2: 0.6708\n",
      "Epoch 5/20\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.5696 - accuracy: 0.8351 - precision_2: 0.8898 - recall_2: 0.7641\n",
      "Epoch 6/20\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.8671 - precision_2: 0.9121 - recall_2: 0.8156\n",
      "Epoch 7/20\n",
      "144/144 [==============================] - 0s 660us/step - loss: 0.3737 - accuracy: 0.8900 - precision_2: 0.9314 - recall_2: 0.8594\n",
      "Epoch 8/20\n",
      "144/144 [==============================] - 0s 673us/step - loss: 0.3132 - accuracy: 0.9095 - precision_2: 0.9369 - recall_2: 0.8887\n",
      "Epoch 9/20\n",
      "144/144 [==============================] - 0s 653us/step - loss: 0.2732 - accuracy: 0.9228 - precision_2: 0.9474 - recall_2: 0.9019\n",
      "Epoch 10/20\n",
      "144/144 [==============================] - 0s 667us/step - loss: 0.2383 - accuracy: 0.9325 - precision_2: 0.9542 - recall_2: 0.9137\n",
      "Epoch 11/20\n",
      "144/144 [==============================] - 0s 783us/step - loss: 0.2142 - accuracy: 0.9429 - precision_2: 0.9616 - recall_2: 0.9248\n",
      "Epoch 12/20\n",
      "144/144 [==============================] - 0s 859us/step - loss: 0.1992 - accuracy: 0.9527 - precision_2: 0.9636 - recall_2: 0.9388\n",
      "Epoch 13/20\n",
      "144/144 [==============================] - 0s 928us/step - loss: 0.1832 - accuracy: 0.9527 - precision_2: 0.9644 - recall_2: 0.9415 0s - loss: 0.1380 - accuracy: 0.9685 - precision_2: 0.9811 - recall_2: 0.96\n",
      "Epoch 14/20\n",
      "144/144 [==============================] - 0s 741us/step - loss: 0.1669 - accuracy: 0.9631 - precision_2: 0.9736 - recall_2: 0.9492\n",
      "Epoch 15/20\n",
      "144/144 [==============================] - 0s 718us/step - loss: 0.1562 - accuracy: 0.9645 - precision_2: 0.9744 - recall_2: 0.9555\n",
      "Epoch 16/20\n",
      "144/144 [==============================] - 0s 700us/step - loss: 0.1421 - accuracy: 0.9666 - precision_2: 0.9753 - recall_2: 0.9617\n",
      "Epoch 17/20\n",
      "144/144 [==============================] - 0s 838us/step - loss: 0.1307 - accuracy: 0.9708 - precision_2: 0.9780 - recall_2: 0.9596\n",
      "Epoch 18/20\n",
      "144/144 [==============================] - 0s 796us/step - loss: 0.1252 - accuracy: 0.9673 - precision_2: 0.9781 - recall_2: 0.9617\n",
      "Epoch 19/20\n",
      "144/144 [==============================] - 0s 693us/step - loss: 0.1123 - accuracy: 0.9722 - precision_2: 0.9775 - recall_2: 0.9680\n",
      "Epoch 20/20\n",
      "144/144 [==============================] - 0s 665us/step - loss: 0.1075 - accuracy: 0.9729 - precision_2: 0.9823 - recall_2: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a37d515a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16 , input_dim =X_train.shape[1], activation ='relu'))\n",
    "model.add(Dense(8 , activation ='relu'))\n",
    "model.add(Dense(digits.target_names.shape[0], activation ='softmax'))\n",
    "model.compile(loss ='categorical_crossentropy', optimizer='adam', metrics =['accuracy',Precision(),Recall()])\n",
    "model.fit (X_train, train_labels, epochs=20, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec6947e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9472 - precision_2: 0.9576 - recall_2: 0.9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20150932669639587,\n",
       " 0.9472222328186035,\n",
       " 0.9576271176338196,\n",
       " 0.9416666626930237]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "model.evaluate(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c06e0f",
   "metadata": {},
   "source": [
    "# Fully connected Model for the **Faces Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c879c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces.data\n",
    "y = faces.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "train_labels = to_categorical(y_train, 40)\n",
    "test_labels = to_categorical(y_test, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6b36aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1181 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.0188 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.0125 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0250 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.0125 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0344 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0375 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.0344 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.0344 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.0281 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0281 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0281 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0219 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0250 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0188 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0188 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.0188 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.0156 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.0281 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.0156 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.0312 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a37ce7f8e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16 , input_dim =X_train.shape[1], activation ='relu'))\n",
    "model.add(Dense(8 , activation ='relu'))\n",
    "model.add(Dense(40, activation ='softmax'))\n",
    "model.compile(loss ='binary_crossentropy', optimizer='adam', metrics =['accuracy',Precision(),Recall()])\n",
    "model.fit (X_train, train_labels, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb41ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1187 - accuracy: 0.0000e+00 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11867066472768784, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "model.evaluate(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85334b",
   "metadata": {},
   "source": [
    "As we can see, the fully connected neural network doesn't perform well on the faces dataset, on one hand there are many classes and not too much data but on the other hand FC models do not take the spacial caractersitics into consideration, that's why we will try training the dataset on a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e87ec5",
   "metadata": {},
   "source": [
    "## Implementing VGG 16 and training faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0221a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces['images']\n",
    "y = faces['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
    "train_labels = to_categorical(y_train, num_classes=40)\n",
    "test_labels = to_categorical(y_test, num_classes=40)\n",
    "X_train=np.dstack([X_train] * 3)\n",
    "X_test=np.dstack([X_test]*3)\n",
    "X_train = X_train.reshape(-1, 64,64,3)\n",
    "X_test= X_test.reshape (-1,64,64,3)\n",
    "\n",
    "vgg_16_model = Sequential()\n",
    "vgg_16_model.add(Conv2D(input_shape=(64,64,3) ,filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_16_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_16_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_16_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_16_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_16_model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_16_model.add(Flatten())\n",
    "vgg_16_model.add(Dense(units=4096,activation=\"relu\"))\n",
    "vgg_16_model.add(Dense(units=4096,activation=\"relu\"))\n",
    "vgg_16_model.add(Dense(units=40, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "124d7cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 64, 64, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7c35b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 23s 2s/step - loss: 3.6965 - accuracy: 0.0156 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 3.6901 - accuracy: 0.0312 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 3.6856 - accuracy: 0.0219 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 3.6841 - accuracy: 0.0344 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 3.6835 - accuracy: 0.0312 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a312347ee0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_16_model.compile(loss ='categorical_crossentropy', optimizer='adam', metrics =['accuracy',Precision(),Recall()])\n",
    "vgg_16_model.fit (X_train, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5787bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "WARNING:tensorflow:5 out of the last 22 calls to <function Model.make_test_function.<locals>.test_function at 0x000001A31234D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 3.8029 - accuracy: 0.0000e+00 - precision_15: 0.0000e+00 - recall_15: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.8028578758239746, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "vgg_16_model.evaluate(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1d456",
   "metadata": {},
   "source": [
    "As we can see, VGG16 didn't better results for faces dataset compared to the FC neural network.\n",
    "One other statement we can add is the long time can such a big and complex architecture take for training.\n",
    "We will try to remidate to this problem using transfer learning and fine tune a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e216a",
   "metadata": {},
   "source": [
    "### Fine tuning a pretrained model VGG 16 with faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2f84ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces['images']\n",
    "y = faces['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
    "train_labels = to_categorical(y_train, num_classes=40)\n",
    "test_labels = to_categorical(y_test, num_classes=40)\n",
    "X_train=np.dstack([X_train] * 3)\n",
    "X_test=np.dstack([X_test]*3)\n",
    "X_train = X_train.reshape(-1, 64,64,3)\n",
    "X_test= X_test.reshape (-1,64,64,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "444349ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "input_layer=layers.Input(shape=(64,64,3))\n",
    "model_vgg16=VGG16(weights='imagenet',input_tensor=input_layer,include_top=False)\n",
    "last_layer=model_vgg16.output\n",
    "flatten=layers.Flatten()(last_layer) \n",
    "\n",
    "# Add dense layer\n",
    "dense1=layers.Dense(100,activation='relu')(flatten)\n",
    "dense1=layers.Dense(100,activation='relu')(flatten)\n",
    "dense1=layers.Dense(100,activation='relu')(flatten)\n",
    "\n",
    "output_layer=layers.Dense(40,activation='softmax')(flatten)\n",
    "model=models.Model(inputs=input_layer,outputs=output_layer)\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57b772ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 5s 491ms/step - loss: 3.8060 - accuracy: 0.0844 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 5s 487ms/step - loss: 2.9871 - accuracy: 0.3375 - precision_12: 0.0000e+00 - recall_12: 0.0000e+00\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 5s 541ms/step - loss: 2.4208 - accuracy: 0.7094 - precision_12: 1.0000 - recall_12: 0.0063\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 1.9524 - accuracy: 0.8125 - precision_12: 1.0000 - recall_12: 0.0188\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 6s 592ms/step - loss: 1.5825 - accuracy: 0.8719 - precision_12: 1.0000 - recall_12: 0.0594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a30d09f820>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss ='categorical_crossentropy', optimizer='adam', metrics =['accuracy',Precision(),Recall()])\n",
    "model.fit (X_train, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05943066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 40)                81960     \n",
      "=================================================================\n",
      "Total params: 14,796,648\n",
      "Trainable params: 81,960\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d5eca80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 1.7037 - accuracy: 0.7000 - precision_12: 1.0000 - recall_12: 0.0625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7037107944488525, 0.699999988079071, 1.0, 0.0625]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "model.evaluate(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28670b11",
   "metadata": {},
   "source": [
    "## Fine tuning a pretrained model resnet avec faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b72c870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "X = faces['images']\n",
    "x = keras.applications.resnet_v2.preprocess_input(tf.cast(X, tf.float32))\n",
    "y = faces['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
    "train_labels = to_categorical(y_train, num_classes=40)\n",
    "test_labels = to_categorical(y_test, num_classes=40)\n",
    "x = tf.image.grayscale_to_rgb(tf.constant(X_train))\n",
    "\n",
    "model = keras.Sequential()\n",
    "base_model = keras.applications.ResNet152V2(\n",
    "  include_top=False,\n",
    "  weights='imagenet',\n",
    "  input_shape=(64, 64, 3)\n",
    ")\n",
    "for layer in base_model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(40, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "245ccd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.dstack([X_train] * 3)\n",
    "X_test=np.dstack([X_test]*3)\n",
    "X_train = X_train.reshape(-1, 64,64,3)\n",
    "X_test= X_test.reshape (-1,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f5e1eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 5s 548ms/step - loss: 0.0299 - accuracy: 1.0000 - precision_10: 1.0000 - recall_10: 1.0000\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 5s 540ms/step - loss: 0.0251 - accuracy: 1.0000 - precision_10: 1.0000 - recall_10: 1.0000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 6s 608ms/step - loss: 0.0218 - accuracy: 1.0000 - precision_10: 1.0000 - recall_10: 1.0000\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 0.0192 - accuracy: 1.0000 - precision_10: 1.0000 - recall_10: 1.0000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 6s 631ms/step - loss: 0.0168 - accuracy: 1.0000 - precision_10: 1.0000 - recall_10: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a31096e880>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss ='categorical_crossentropy', optimizer='adam', metrics =['accuracy',Precision(),Recall()])\n",
    "model.fit (X_train, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98991675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet152v2 (Functional)     (None, 2, 2, 2048)        58331648  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 40)                327720    \n",
      "=================================================================\n",
      "Total params: 58,659,368\n",
      "Trainable params: 331,816\n",
      "Non-trainable params: 58,327,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0d66778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 0.5030 - accuracy: 0.8750 - precision_10: 0.9286 - recall_10: 0.8125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5030378103256226, 0.875, 0.9285714030265808, 0.8125]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "model.evaluate(X_test,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6814d2",
   "metadata": {},
   "source": [
    "#### Interpretation :  \n",
    "As we can see VGG16 and Resnet50_v2 on faces dataset have very close results in terms of accuracy on test set and they both outperform the hard-coded vgg16 architecture. In addition, we spend less time on training them since we have used frozen some layers and as a result there is no need to train all the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d08ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
