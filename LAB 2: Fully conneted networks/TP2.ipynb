{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wg_HWBXC8hQ"
   },
   "source": [
    "# Environement setup\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUwcBtHLWOJo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lCT2n7woX6K"
   },
   "source": [
    "# Practice questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TElSoLvyDDVu"
   },
   "source": [
    "### Question 1: Gradient descient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkUMpa43c5Tr"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return x*x - 1\n",
    "\n",
    "def f2(x1, x2):\n",
    "  return 1*(x1**2) + 2*x2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fyo9HmQrIRq-",
    "outputId": "c92227d2-a7d2-4099-9957-83b2c95a72c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minima of y= x + 2*z - 1\n",
      "Converged with:  [0.0, -199.9981]\n",
      "\n",
      "Minima of y=x²\n",
      "Converged with 3.6185043e-08\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_one_variable(f, initial_x, max_iters=100, lr=0.01, precision = 0.000001, round_to=0.001):\n",
    "  cur_x = initial_x \n",
    "  previous_step_size = 1\n",
    "  iters = 0\n",
    "\n",
    "  for iters in range(max_iters):\n",
    "    print(\"iteration\",iters)\n",
    "    prev_x = cur_x \n",
    "    cur_x = cur_x - lr * f(prev_x) \n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    \n",
    "    if previous_step_size <= precision :\n",
    "      print(\"Converged with\", x.numpy())\n",
    "      return cur_x\n",
    "\n",
    "  print(\"Exited with\", cur_x)\n",
    "  return cur_x\n",
    "      \n",
    "      \n",
    "def tf_gradient_descent_one_variable(f, initial_x, max_iters=1000, lr=0.01, precision = 0.001, optimizer=tf.keras.optimizers.SGD):\n",
    "  x = tf.Variable(1.0*initial_x)\n",
    "  opt = optimizer(learning_rate=lr)\n",
    "  prev_x = x.numpy()\n",
    "  \n",
    "  for i in range(max_iters):\n",
    "    \n",
    "    # https://medium.com/analytics-vidhya/3-different-ways-to-perform-gradient-descent-in-tensorflow-2-0-and-ms-excel-ffc3791a160a\n",
    "    # method 2\n",
    "    with tf.GradientTape() as tape:\n",
    "      y = f(x)\n",
    "    grads = tape.gradient(y, [x])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [x])\n",
    "    opt.apply_gradients(grads_and_vars)\n",
    "    previous_step_size = abs(x.numpy() - prev_x)\n",
    "    \n",
    "    prev_x = x.numpy()\n",
    "    if previous_step_size <= precision :\n",
    "      print(\"Converged with\", x.numpy())\n",
    "      return x.numpy()\n",
    "\n",
    "  print(\"Exited with\", x.numpy())\n",
    "  return x.numpy()\n",
    "\n",
    "def tf_gradient_descent_n_variables(f, initial_variables, max_iters=1000, lr=0.01, precision = 0.001, optimizer=tf.keras.optimizers.SGD):\n",
    "  X = [ tf.Variable(1.0* variable) for variable in initial_variables ]\n",
    "  opt = optimizer(learning_rate=lr)\n",
    "  \n",
    "  for i in range(max_iters):\n",
    "    with tf.GradientTape() as tape:\n",
    "      y = f(*X)  # <=> X.unpack()\n",
    "    grads = tape.gradient(y, X)\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, X)\n",
    "    opt.apply_gradients(grads_and_vars)\n",
    "    \n",
    "  variables = [v.numpy() for v in X]\n",
    "  print(\"Converged with: \", variables)\n",
    "  return variables\n",
    "\n",
    "\n",
    "print(\"\\nMinima of y= x + 2*z - 1\")\n",
    "minimum = tf_gradient_descent_n_variables(f2, [5, 0], lr=0.1, precision=0.001)\n",
    "\n",
    "print(\"\\nMinima of y=x²\")\n",
    "minimum = tf_gradient_descent_one_variable(f, 5, lr=0.1, precision=10**-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM3jbwc7rGgV"
   },
   "outputs": [],
   "source": [
    "class GradientDescent():\n",
    "  def __init__(self, X, y):\n",
    "    self.X = np.array(X)\n",
    "    self.y = np.array(y)\n",
    "    self.weights = [1.0] + [1.0 for _ in X[0] ]\n",
    "\n",
    "  def __call__(self, X):\n",
    "    return self.predict(X)\n",
    "\n",
    "  def predict(self, X):\n",
    "    assert len(X) == len(self.weights) - 1\n",
    "    X = X\n",
    "    return sum(i[0] * i[1] for i in zip(X, self.weights))\n",
    "\n",
    "  def loss(self, weights):\n",
    "    predictions = []\n",
    "    i = 0\n",
    "    for x in self.X:\n",
    "      x = [1.0] + x\n",
    "      predictions.append(sum(i[0] * i[1] for i in zip(x, weights)) )\n",
    "    \n",
    "    MAE = 0\n",
    "    for i in range(len(predictions)):\n",
    "      MAE += abs(predictions[i] - self.y[i] )\n",
    "    return MAE\n",
    "    \n",
    "\n",
    "  def train(self, max_iters=100, lr=0.01, epochs=2, precision = 0.001, optimizer=tf.keras.optimizers.SGD):\n",
    "    opt = optimizer(learning_rate=lr)\n",
    "    X = [tf.Variable(1.0* variable) for variable in self.weights ]\n",
    "    # Compute the gradients for a list of variables.\n",
    "    for i in range(max_iters):\n",
    "      with tf.GradientTape() as tape:\n",
    "        y = self.loss(X) # has to be a one line loss function...\n",
    "      grads = tape.gradient(y, X)\n",
    "      # Process the gradients, for example cap them, etc.\n",
    "      # capped_grads = [MyCapper(g) for g in grads]\n",
    "      processed_grads = [g for g in grads]\n",
    "      # Ask the optimizer to apply the processed gradients.\n",
    "      opt.apply_gradients(\n",
    "          (grad, var) \n",
    "          for (grad, var) in zip(processed_grads, X)\n",
    "          if grad is not None\n",
    "          )\n",
    "      variables = [ v.numpy() for v in X ]\n",
    "      self.weights = variables\n",
    "    return variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKC5JbE7N74M"
   },
   "source": [
    "## Question 2: optimizers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bts55U-5HP8y",
    "outputId": "aad29c59-6b04-434b-b7fc-ae2469f12934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Converged with -2.0924533e-06\n",
      "RMSprop\n",
      "Converged with 4.97395e-10\n",
      "Momentum\n",
      "Converged with 3.6185043e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"Adam\")\n",
    "minimum = tf_gradient_descent_one_variable(f, 5, lr=0.1, precision=10**-8, optimizer=tf.keras.optimizers.Adam)\n",
    "print(\"RMSprop\")\n",
    "minimum = tf_gradient_descent_one_variable(f, 5, lr=0.1, precision=10**-8, optimizer=tf.keras.optimizers.RMSprop)\n",
    "print(\"Momentum\")\n",
    "minimum = tf_gradient_descent_one_variable(f, 5, lr=0.1, precision=10**-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpPN6zHHTYii"
   },
   "source": [
    "## 2. I implement in order to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrsdDDHkZWei"
   },
   "outputs": [],
   "source": [
    "# perceptron from tp1\n",
    "def perceptron(x, y, max_iterations=3000):\n",
    "  w = np.zeros((1,2))\n",
    "  p = np.zeros((1,2))\n",
    "  for it in range(max_iterations):\n",
    "        for X, Y in zip(x, y) :\n",
    "            if  Y * np.inner(X, w) <= 0 : \n",
    "                p = w\n",
    "                w = w + np.multiply(X, Y)\n",
    "        if (w == p).all() :\n",
    "            return (w.tolist(), True)\n",
    "  return (w.tolist(), it>=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1UOEQYpaS6F"
   },
   "source": [
    "### 2.1 Toy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xVNiX-vOs9S"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(sigma1, sigma2, test_size=0.2):\n",
    "  mu1 = [-1, 0]\n",
    "  mu2 = [1, 0]\n",
    "  \n",
    "  cov1 = [ [sigma1 , 0 ], \n",
    "           [0 , sigma1 ] \n",
    "          ]\n",
    "\n",
    "  cov2 = [  [ sigma2 , 0 ], \n",
    "            [ 0 , sigma2 ] \n",
    "          ] \n",
    "  x1 = multivariate_normal(mu1, cov1, 125)\n",
    "  x2 = multivariate_normal(mu2, cov2, 125)\n",
    "  dataset = pd.DataFrame({\n",
    "      \"x\": x1.tolist() + x2.tolist(),\n",
    "      \"y\": [-1 for _ in range(125)] + [1 for _ in range(125)]\n",
    "  })\n",
    "\n",
    "  return train_test_split(dataset.get(\"x\"), dataset.get(\"y\"), test_size=test_size, random_state=42)\n",
    "\n",
    "# use:\n",
    "train_x, test_x, train_y, test_y = generate_dataset(sigma1=0.1, sigma2=0.01)\n",
    "dataset_x = (train_x, test_x)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WEaYSHyM0tn",
    "outputId": "f82cf0a8-beda-4391-e881-0abcf2e485bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 36s, sys: 2.24 s, total: 2min 38s\n",
      "Wall time: 2min 38s\n",
      "Gradient descent converges with:  [1.6695287, 0.036065035, 1.0]\n"
     ]
    }
   ],
   "source": [
    "grad = GradientDescent(train_x.tolist(), train_y.tolist())\n",
    "%time w = grad.train(lr=learning_rate, max_iters=max_iterations)\n",
    "\n",
    "print(\"Gradient descent converges with: \", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJy8emuanf8S",
    "outputId": "395e6ce7-4b29-4568-b524-12fd10a47692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 143 ms, total: 1.5 s\n",
      "Wall time: 1.35 s\n",
      "minima found:  [[0.8343986857525884, 0.02353063923702465]]\n"
     ]
    }
   ],
   "source": [
    "%time w, converges = perceptron(train_x, train_y, max_iterations=max_iterations)\n",
    "\n",
    "if converges :\n",
    "  print(\"Perceptron converged: \")\n",
    "print(\"minima found: \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICLj7PztYc2c"
   },
   "source": [
    "## 2.2  Real data: IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0XRrKx0T2gt"
   },
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "v3PC3QUimwcg",
    "outputId": "e034ab34-2825-4bc3-bc7e-1acf71e4a252"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  ...  petal width (cm)      target\n",
       "count         150.000000        150.000000  ...        150.000000  150.000000\n",
       "mean            5.843333          3.057333  ...          1.199333    1.000000\n",
       "std             0.828066          0.435866  ...          0.762238    0.819232\n",
       "min             4.300000          2.000000  ...          0.100000    0.000000\n",
       "25%             5.100000          2.800000  ...          0.300000    0.000000\n",
       "50%             5.800000          3.000000  ...          1.300000    1.000000\n",
       "75%             6.400000          3.300000  ...          1.800000    2.000000\n",
       "max             7.900000          4.400000  ...          2.500000    2.000000\n",
       "\n",
       "[8 rows x 5 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G1FZ17tmxs5",
    "outputId": "77520eab-330b-4301-fe34-6fdea20f371c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (150, 3)\n",
      "output shape:(150,)\n"
     ]
    }
   ],
   "source": [
    "encoder =  LabelEncoder()\n",
    "\n",
    "iris_X = iris_df.iloc[:,0:3].values\n",
    "iris_y = encoder.fit_transform(iris_df.iloc[:,4].values)\n",
    "\n",
    "print(\"input: {}\\noutput shape:{}\".format(iris_X.shape, iris_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWvlucY8VB64",
    "outputId": "a6cb2a2d-7454-4220-dc9a-6e7160fc18a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "print(set(iris_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2PEk84OVn7h"
   },
   "source": [
    "### This is a multi class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVlvROmIVhd8"
   },
   "outputs": [],
   "source": [
    "y_dummies = pd.get_dummies(iris_y).values # encoding in ternary: 0 -> 100, 1 -> 010 and 2 -> 001\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, y_dummies, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auM4aZdDYstI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5YY7BWfWJ1U"
   },
   "source": [
    "### Model 1: Simple, plain, unimpressive neural net (Linear correlation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAkLYIPtV0qA"
   },
   "outputs": [],
   "source": [
    "stupid_NN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NrUEu6nWVR5"
   },
   "source": [
    "### Model 2: Neural net with hidden layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpcLR5aDWTs6"
   },
   "outputs": [],
   "source": [
    "long_NN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnx8ojTTWpQx"
   },
   "source": [
    "### Model 3: One layer neural net with more neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXVvqgs1WgKc"
   },
   "outputs": [],
   "source": [
    "thicc_NN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKnpDiErXdvX"
   },
   "source": [
    "### Model 4: One layer neural net with softmax instead of relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-D43mDtXjIV"
   },
   "outputs": [],
   "source": [
    "softmax_NN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGf6vqNoaSgT"
   },
   "source": [
    "# Model 5: Everything combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Vi-ED0jaWG0"
   },
   "outputs": [],
   "source": [
    "chad_NN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pHwt32vW4Lc"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvSltXcHXJQo"
   },
   "outputs": [],
   "source": [
    "# Evaluation method for the models:\n",
    "def evaluate(model, lr=0.01, opt='rmsprop', loss='categorical_crossentropy'):\n",
    "  model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  model.fit(X_train, y_train, batch_size=50, epochs=100, verbose=0)\n",
    "  loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "  print('Test loss:', loss)\n",
    "  print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eqq3wQWZ3kS"
   },
   "source": [
    "#### Experiment 1: \n",
    "The learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EzzJw2bW1U-",
    "outputId": "a2bd61cb-1d7c-46b5-832b-4901da23d9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.1\n",
      "Test loss: 8.05904769897461\n",
      "Test accuracy: 0.20000000298023224\n",
      "\n",
      "Learning rate:  0.01\n",
      "Test loss: 8.05904769897461\n",
      "Test accuracy: 0.20000000298023224\n",
      "\n",
      "Learning rate:  0.001\n",
      "Test loss: 8.05904769897461\n",
      "Test accuracy: 0.20000000298023224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [0.1, 0.01, 0.001]:\n",
    "  print(\"Learning rate: \",learning_rate)\n",
    "  evaluate(stupid_NN, lr=learning_rate)\n",
    "  print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FVEQVziaAZi"
   },
   "source": [
    "#### Experiment 2: \n",
    "The number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWXeGjDkX2eK",
    "outputId": "236f0012-452c-4e11-dd90-79df535472be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer:\n",
      "Test loss: 8.05904769897461\n",
      "Test accuracy: 0.20000000298023224\n",
      "\n",
      "2 layers:\n",
      "Test loss: 10.20812702178955\n",
      "Test accuracy: 0.36666667461395264\n"
     ]
    }
   ],
   "source": [
    "print(\"1 layer:\")\n",
    "evaluate(stupid_NN)\n",
    "print(\"\\n2 layers:\")\n",
    "evaluate(long_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJNCFcGZaLVD"
   },
   "source": [
    "#### Experiment 3: \n",
    "The number of neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFMylk1CaJse",
    "outputId": "1513ae29-32cc-4f5f-d29e-049a8b565622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer - 256 neurons:\n",
      "Test loss: 8.05904769897461\n",
      "Test accuracy: 0.20000000298023224\n",
      "\n",
      "1 layers - 512 neurons:\n",
      "Test loss: 6.984508037567139\n",
      "Test accuracy: 0.9666666388511658\n",
      "\n",
      "2 layers - 512 and 256 neurons:\n",
      "Test loss: 5.90996789932251\n",
      "Test accuracy: 0.36666667461395264\n"
     ]
    }
   ],
   "source": [
    "print(\"1 layer - 256 neurons:\")\n",
    "evaluate(stupid_NN)\n",
    "print(\"\\n1 layers - 512 neurons:\")\n",
    "evaluate(thicc_NN)\n",
    "print(\"\\n2 layers - 512 and 256 neurons:\")\n",
    "evaluate(chad_NN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMWUvLfIaw25"
   },
   "source": [
    "#### Experiment 4: \n",
    "The error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bYwYJ0eatYb",
    "outputId": "fc0b2261-e81f-4c23-dbdc-34b36308329f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 layer - 512 neurons - Categorical crossentropy:\n",
      "Test loss: 6.984508037567139\n",
      "Test accuracy: 0.9666666388511658\n",
      "\n",
      "1 layer - 512 neurons - Categorical cringe:\n",
      "Test loss: 6.984508037567139\n",
      "Test accuracy: 0.9666666388511658\n",
      "\n",
      "1 layer - 512 neurons - Cosine similarity:\n",
      "Test loss: 6.984508037567139\n",
      "Test accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1 layer - 512 neurons - Categorical crossentropy:\")\n",
    "evaluate(thicc_NN, loss='cateogorical_crossentropy')\n",
    "print(\"\\n1 layer - 512 neurons - Categorical cringe:\")\n",
    "evaluate(thicc_NN, loss='cateogorical_hinge')\n",
    "print(\"\\n1 layer - 512 neurons - Cosine similarity:\")\n",
    "evaluate(thicc_NN, loss='cosine_similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0O67WXdbrVu"
   },
   "source": [
    "# Experiment 5: the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMGe6TsNbNjf",
    "outputId": "390b0d5b-2007-404c-fb8d-dfaee95ceff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 layer - 512 neurons - Relu:\n",
      "Test loss: 6.984508037567139\n",
      "Test accuracy: 0.9666666388511658\n",
      "\n",
      "1 layer - 512 neurons - Softmax:\n",
      "Test loss: 12.894476890563965\n",
      "Test accuracy: 0.4333333373069763\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1 layer - 512 neurons - Relu:\")\n",
    "evaluate(thicc_NN)\n",
    "print(\"\\n1 layer - 512 neurons - Softmax:\")\n",
    "evaluate(softmax_NN)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
